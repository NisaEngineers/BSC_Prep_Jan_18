{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f0f344f0617443cb37a4a2e66205533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4456fad6c49b40048ecc05f048b05528",
              "IPY_MODEL_3478f8fb13e94d7a9f721f3c435e18bc",
              "IPY_MODEL_66bfe5a87ae7402bacacdd4aab515be7"
            ],
            "layout": "IPY_MODEL_e12a7fc6887b406da13ad1f619692556"
          }
        },
        "4456fad6c49b40048ecc05f048b05528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_571a06f6a8994b7fa265384b72c05414",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a5c8439137a548f3aa11c7cd635ecfdc",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "3478f8fb13e94d7a9f721f3c435e18bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ebf623541fe4aa585ef7eed3c15f979",
            "max": 338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0817fd887992452e9adf0a62d511f975",
            "value": 338
          }
        },
        "66bfe5a87ae7402bacacdd4aab515be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4bd9c63de1846e78a8fc83484ba861c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cdc2a4eafee84df68db68c197a661aa8",
            "value": "â€‡338/338â€‡[00:01&lt;00:00,â€‡417.24it/s,â€‡Materializingâ€‡param=model.norm.weight]"
          }
        },
        "e12a7fc6887b406da13ad1f619692556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571a06f6a8994b7fa265384b72c05414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c8439137a548f3aa11c7cd635ecfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ebf623541fe4aa585ef7eed3c15f979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0817fd887992452e9adf0a62d511f975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4bd9c63de1846e78a8fc83484ba861c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc2a4eafee84df68db68c197a661aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b82a097b5322436aaabe574478c66b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0ab3315e9d342028e25148ccb6a7447",
              "IPY_MODEL_652507750bfa4491ab694cd537c628e1",
              "IPY_MODEL_c4684b5da6474c2ebb4a88dad9e46d3e"
            ],
            "layout": "IPY_MODEL_89edbd3468104639afd313fea4263f7d"
          }
        },
        "c0ab3315e9d342028e25148ccb6a7447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_398ebd1c3bb14392aab7e5a566ae09d1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a82209d733d74455945865a82543b346",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "652507750bfa4491ab694cd537c628e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f28826142264e82bd967edf388b461e",
            "max": 338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9044992109d14587aef5c49246e0b405",
            "value": 338
          }
        },
        "c4684b5da6474c2ebb4a88dad9e46d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26bacfead9c14c38b8f1dc79916b971a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_52b8e682b55548b3bba56009d81f8172",
            "value": "â€‡338/338â€‡[00:01&lt;00:00,â€‡371.02it/s,â€‡Materializingâ€‡param=model.norm.weight]"
          }
        },
        "89edbd3468104639afd313fea4263f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "398ebd1c3bb14392aab7e5a566ae09d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82209d733d74455945865a82543b346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f28826142264e82bd967edf388b461e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9044992109d14587aef5c49246e0b405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26bacfead9c14c38b8f1dc79916b971a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52b8e682b55548b3bba56009d81f8172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm3AJB8EYBP6",
        "outputId": "74f2228c-4b22-4575-8544-abca1af896ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MCQ Generator using Gemma or Qwen text generation models.\n",
        "Designed to run in Google Colab.\n",
        "\n",
        "Usage:\n",
        "  - Set MODEL_ID to a Gemma or Qwen model (see options below).\n",
        "  - Paste your input text into INPUT_TEXT.\n",
        "  - (Optional) Provide a custom pattern in QUESTION_PATTERN.\n",
        "  - Run the script and answer each question interactively.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1.  INSTALL DEPENDENCIES  (run once in Colab)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# !pip install -q transformers accelerate bitsandbytes\n",
        "\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2.  CONFIGURATION  â€“ edit these as needed\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# --- Model options (pick one) ---\n",
        "# Qwen  (recommended for Colab free tier â€“ smaller & fast):\n",
        "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "# Other options:\n",
        "# MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "# MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"       # needs ~16 GB VRAM\n",
        "# MODEL_ID = \"google/gemma-2-2b-it\"            # requires HF token\n",
        "# MODEL_ID = \"google/gemma-2-9b-it\"            # needs ~20 GB VRAM\n",
        "\n",
        "# --- Number of questions to generate ---\n",
        "NUM_QUESTIONS = 5\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3.  INPUT TEXT  â€“ paste the text you want to\n",
        "#     turn into MCQs here\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "INPUT_TEXT = \"\"\"\n",
        "Photosynthesis is the process used by plants, algae, and certain bacteria to\n",
        "harness energy from sunlight and turn it into chemical energy. During\n",
        "photosynthesis, plants absorb carbon dioxide (CO2) from the air through tiny\n",
        "pores called stomata and water (H2O) from the soil through their roots.\n",
        "Using the energy from sunlight, the plant converts these raw materials into\n",
        "glucose (C6H12O6) and oxygen (O2). The overall chemical equation is:\n",
        "  6CO2 + 6H2O + light energy â†’ C6H12O6 + 6O2\n",
        "Photosynthesis occurs mainly in the chloroplasts, where the green pigment\n",
        "chlorophyll absorbs light energy. The process has two main stages: the\n",
        "light-dependent reactions (which capture solar energy and produce ATP and\n",
        "NADPH) and the Calvin cycle (which uses that energy to fix CO2 into glucose).\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 4.  OPTIONAL PATTERN  â€“ describe any extra\n",
        "#     structure you want the questions to follow.\n",
        "#     Leave empty (\"\") for the default style.\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "QUESTION_PATTERN = \"\"\"\n",
        "- Focus on factual recall and conceptual understanding.\n",
        "- Avoid trivial or trick questions.\n",
        "- Distractors (wrong options) should be plausible but clearly incorrect.\n",
        "- The difficulty should be appropriate for a high-school or undergraduate level.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 5.  HELPER FUNCTIONS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def build_prompt(text: str, pattern: str, n: int) -> str:\n",
        "    \"\"\"Build the instruction prompt sent to the model.\"\"\"\n",
        "    pattern_block = (\n",
        "        f\"\\nAdditional requirements for the questions:\\n{pattern.strip()}\\n\"\n",
        "        if pattern.strip() else \"\"\n",
        "    )\n",
        "\n",
        "    return textwrap.dedent(f\"\"\"\n",
        "        You are an expert educator. Read the following text and generate exactly\n",
        "        {n} multiple-choice questions (MCQs) that test understanding of the content.\n",
        "        {pattern_block}\n",
        "        Return your answer as a **valid JSON array** with no extra text before or\n",
        "        after it.  Each element must follow this exact schema:\n",
        "\n",
        "        {{\n",
        "          \"question\": \"<question text>\",\n",
        "          \"options\": {{\n",
        "            \"a\": \"<option A>\",\n",
        "            \"b\": \"<option B>\",\n",
        "            \"c\": \"<option C>\",\n",
        "            \"d\": \"<option D>\"\n",
        "          }},\n",
        "          \"answer\": \"<correct letter: a | b | c | d>\",\n",
        "          \"explanation\": \"<brief explanation of why the answer is correct>\"\n",
        "        }}\n",
        "\n",
        "        ===TEXT START===\n",
        "        {text.strip()}\n",
        "        ===TEXT END===\n",
        "\n",
        "        JSON array:\n",
        "    \"\"\").strip()\n",
        "\n",
        "\n",
        "def load_model(model_id: str):\n",
        "    \"\"\"Load the tokenizer and model with optional 4-bit quantisation.\"\"\"\n",
        "    print(f\"\\nâ³  Loading model: {model_id}  â€¦\")\n",
        "\n",
        "    # Use 4-bit quantisation when a CUDA GPU is available to save VRAM.\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    bnb_config = None\n",
        "    if use_cuda:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\" if use_cuda else None,\n",
        "        torch_dtype=torch.bfloat16 if use_cuda else torch.float32,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model.eval()\n",
        "    print(\"âœ…  Model loaded.\\n\")\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def generate_mcqs(tokenizer, model, prompt: str, max_new_tokens: int = 2048) -> str:\n",
        "    \"\"\"Run inference and return the raw model output string.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # --- tokenise input -------------------------------------------------------\n",
        "    # apply_chat_template may return a plain Tensor OR a BatchEncoding object\n",
        "    # (which is NOT a plain dict, so isinstance(x, dict) misses it).\n",
        "    # Safest approach: always ask for a list of ints, then convert manually.\n",
        "    try:\n",
        "        token_list = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=None,      # returns a plain Python list of ints\n",
        "        )\n",
        "        input_ids = torch.tensor([token_list], dtype=torch.long).to(device)\n",
        "    except Exception:\n",
        "        # Fallback: plain tokenisation (no chat template)\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    prompt_len = input_ids.shape[-1]               # save BEFORE generation\n",
        "\n",
        "    # --- generate -------------------------------------------------------------\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,          # greedy â†’ deterministic JSON\n",
        "            temperature=1.0,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode only the newly generated tokens (skip the echoed prompt)\n",
        "    new_tokens = output_ids[0][prompt_len:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def extract_json(raw: str) -> list:\n",
        "    \"\"\"\n",
        "    Extract and parse the first JSON array found in the model output.\n",
        "    Falls back to a regex search if the output contains extra prose.\n",
        "    \"\"\"\n",
        "    # Try direct parse first\n",
        "    raw = raw.strip()\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "\n",
        "    # Search for the first [...] block in the output\n",
        "    match = re.search(r\"\\[.*?\\]\", raw, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            data = json.loads(match.group())\n",
        "            if isinstance(data, list):\n",
        "                return data\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    raise ValueError(\n",
        "        \"Could not parse a JSON array from the model output.\\n\"\n",
        "        f\"Raw output (first 800 chars):\\n{raw[:800]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validate_mcq(mcq: dict, idx: int) -> bool:\n",
        "    \"\"\"Basic sanity check for a single MCQ dict.\"\"\"\n",
        "    required_keys = {\"question\", \"options\", \"answer\", \"explanation\"}\n",
        "    if not required_keys.issubset(mcq.keys()):\n",
        "        print(f\"  âš ï¸  Question {idx+1} is missing keys â€“ skipping.\")\n",
        "        return False\n",
        "    if not isinstance(mcq[\"options\"], dict) or not {\"a\",\"b\",\"c\",\"d\"}.issubset(mcq[\"options\"]):\n",
        "        print(f\"  âš ï¸  Question {idx+1} has malformed options â€“ skipping.\")\n",
        "        return False\n",
        "    if mcq[\"answer\"].lower() not in {\"a\", \"b\", \"c\", \"d\"}:\n",
        "        print(f\"  âš ï¸  Question {idx+1} has an invalid answer key â€“ skipping.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 6.  QUIZ RUNNER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def print_separator(char=\"â”€\", width=60):\n",
        "    print(char * width)\n",
        "\n",
        "\n",
        "def run_quiz(mcqs: list):\n",
        "    \"\"\"Present each MCQ to the user and evaluate their answers.\"\"\"\n",
        "    print_separator(\"â•\")\n",
        "    print(\"ğŸ“  QUIZ TIME!  Answer each question by typing a, b, c, or d.\")\n",
        "    print_separator(\"â•\")\n",
        "\n",
        "    score = 0\n",
        "    total = len(mcqs)\n",
        "\n",
        "    for i, mcq in enumerate(mcqs):\n",
        "        print(f\"\\nQ{i+1} / {total}: {mcq['question']}\\n\")\n",
        "        for letter in [\"a\", \"b\", \"c\", \"d\"]:\n",
        "            print(f\"  {letter})  {mcq['options'][letter]}\")\n",
        "\n",
        "        # Get a valid answer from the user\n",
        "        while True:\n",
        "            raw = input(\"\\nYour answer (a/b/c/d): \").strip().lower()\n",
        "            if raw in {\"a\", \"b\", \"c\", \"d\"}:\n",
        "                break\n",
        "            print(\"  Please type a, b, c, or d.\")\n",
        "\n",
        "        correct = mcq[\"answer\"].lower()\n",
        "\n",
        "        if raw == correct:\n",
        "            score += 1\n",
        "            print(\"\\nâœ…  Correct!\\n\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ  Wrong!  The correct answer was ({correct}) \"\n",
        "                  f\"{mcq['options'][correct]}\")\n",
        "            print(f\"\\nğŸ’¡  Explanation: {mcq['explanation']}\\n\")\n",
        "\n",
        "        print_separator()\n",
        "\n",
        "    # Final score\n",
        "    print(f\"\\nğŸ†  Quiz complete!  You scored {score} / {total}.\")\n",
        "    pct = round(score / total * 100)\n",
        "    if pct == 100:\n",
        "        print(\"    Perfect score â€“ outstanding! ğŸ‰\")\n",
        "    elif pct >= 80:\n",
        "        print(\"    Great job! ğŸ‘\")\n",
        "    elif pct >= 60:\n",
        "        print(\"    Good effort â€“ review the explanations above to improve.\")\n",
        "    else:\n",
        "        print(\"    Keep studying â€“ you'll get there! ğŸ“š\")\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 7.  MAIN\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load model\n",
        "    tokenizer, model = load_model(MODEL_ID)\n",
        "\n",
        "    # Step 2: Build the prompt\n",
        "    prompt = build_prompt(INPUT_TEXT, QUESTION_PATTERN, NUM_QUESTIONS)\n",
        "\n",
        "    # Step 3: Generate MCQs\n",
        "    print(\"âš™ï¸   Generating questions â€¦\\n\")\n",
        "    raw_output = generate_mcqs(tokenizer, model, prompt)\n",
        "\n",
        "    # Step 4: Parse the JSON\n",
        "    try:\n",
        "        mcqs_raw = extract_json(raw_output)\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nâŒ  Error parsing model output:\\n{e}\")\n",
        "        return\n",
        "\n",
        "    # Step 5: Validate and filter\n",
        "    mcqs = [mcq for i, mcq in enumerate(mcqs_raw) if validate_mcq(mcq, i)]\n",
        "\n",
        "    if not mcqs:\n",
        "        print(\"âŒ  No valid questions were generated. \"\n",
        "              \"Try a different model or adjust your input text.\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ…  {len(mcqs)} question(s) generated successfully.\\n\")\n",
        "\n",
        "    # Step 6: Run the interactive quiz\n",
        "    run_quiz(mcqs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4f0f344f0617443cb37a4a2e66205533",
            "4456fad6c49b40048ecc05f048b05528",
            "3478f8fb13e94d7a9f721f3c435e18bc",
            "66bfe5a87ae7402bacacdd4aab515be7",
            "e12a7fc6887b406da13ad1f619692556",
            "571a06f6a8994b7fa265384b72c05414",
            "a5c8439137a548f3aa11c7cd635ecfdc",
            "4ebf623541fe4aa585ef7eed3c15f979",
            "0817fd887992452e9adf0a62d511f975",
            "a4bd9c63de1846e78a8fc83484ba861c",
            "cdc2a4eafee84df68db68c197a661aa8"
          ]
        },
        "id": "ECbtw4OtYM0Z",
        "outputId": "1aa5abd6-0467-4dc3-85c9-b804c8145618"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³  Loading model: Qwen/Qwen2.5-1.5B-Instruct  â€¦\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f0f344f0617443cb37a4a2e66205533"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ…  Model loaded.\n",
            "\n",
            "âš™ï¸   Generating questions â€¦\n",
            "\n",
            "âœ…  5 question(s) generated successfully.\n",
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "ğŸ“  QUIZ TIME!  Answer each question by typing a, b, c, or d.\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "Q1 / 5: What is the primary function of photosynthesis?\n",
            "\n",
            "  a)  To convert light energy into electrical energy\n",
            "  b)  To provide food for animals\n",
            "  c)  To release oxygen into the atmosphere\n",
            "  d)  To store large amounts of water\n",
            "\n",
            "Your answer (a/b/c/d): a\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q2 / 5: In which part of the plant does photosynthesis occur?\n",
            "\n",
            "  a)  Leaves\n",
            "  b)  Stem\n",
            "  c)  Root\n",
            "  d)  All parts equally\n",
            "\n",
            "Your answer (a/b/c/d): a\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q3 / 5: How many molecules of CO2 are required for one molecule of glucose during photosynthesis?\n",
            "\n",
            "  a)  3\n",
            "  b)  4\n",
            "  c)  5\n",
            "  d)  6\n",
            "\n",
            "Your answer (a/b/c/d): 6\n",
            "  Please type a, b, c, or d.\n",
            "\n",
            "Your answer (a/b/c/d): d\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q4 / 5: Which substance is not directly involved in the light-dependent reactions of photosynthesis?\n",
            "\n",
            "  a)  ATP\n",
            "  b)  Water\n",
            "  c)  Glucose\n",
            "  d)  Carbon dioxide\n",
            "\n",
            "Your answer (a/b/c/d): c\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q5 / 5: Why do plants need to take up water from the soil?\n",
            "\n",
            "  a)  To transport nutrients throughout the plant\n",
            "  b)  To cool themselves down\n",
            "  c)  To dissolve sunlight\n",
            "  d)  To collect CO2 from the air\n",
            "\n",
            "Your answer (a/b/c/d): d\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ†  Quiz complete!  You scored 5 / 5.\n",
            "    Perfect score â€“ outstanding! ğŸ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MCQ Generator using Gemma or Qwen text generation models.\n",
        "Designed to run in Google Colab.\n",
        "\n",
        "Usage:\n",
        "  - Set MODEL_ID to a Gemma or Qwen model (see options below).\n",
        "  - Paste your input text into INPUT_TEXT.\n",
        "  - (Optional) Provide a custom pattern in QUESTION_PATTERN.\n",
        "  - Run the script and answer each question interactively.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1.  INSTALL DEPENDENCIES  (run once in Colab)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# !pip install -q transformers accelerate bitsandbytes\n",
        "\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2.  CONFIGURATION  â€“ edit these as needed\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# --- Model options (pick one) ---\n",
        "# Qwen  (recommended for Colab free tier â€“ smaller & fast):\n",
        "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "# Other options:\n",
        "# MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "# MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"       # needs ~16 GB VRAM\n",
        "# MODEL_ID = \"google/gemma-2-2b-it\"            # requires HF token\n",
        "# MODEL_ID = \"google/gemma-2-9b-it\"            # needs ~20 GB VRAM\n",
        "\n",
        "# --- Number of questions to generate ---\n",
        "NUM_QUESTIONS = 5\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3.  INPUT TEXT  â€“ paste the text you want to\n",
        "#     turn into MCQs here\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "INPUT_TEXT = \"\"\"\n",
        "Photosynthesis is the process used by plants, algae, and certain bacteria to\n",
        "harness energy from sunlight and turn it into chemical energy. During\n",
        "photosynthesis, plants absorb carbon dioxide (CO2) from the air through tiny\n",
        "pores called stomata and water (H2O) from the soil through their roots.\n",
        "Using the energy from sunlight, the plant converts these raw materials into\n",
        "glucose (C6H12O6) and oxygen (O2). The overall chemical equation is:\n",
        "  6CO2 + 6H2O + light energy â†’ C6H12O6 + 6O2\n",
        "Photosynthesis occurs mainly in the chloroplasts, where the green pigment\n",
        "chlorophyll absorbs light energy. The process has two main stages: the\n",
        "light-dependent reactions (which capture solar energy and produce ATP and\n",
        "NADPH) and the Calvin cycle (which uses that energy to fix CO2 into glucose).\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 4.  OPTIONAL PATTERN  â€“ describe any extra\n",
        "#     structure you want the questions to follow.\n",
        "#     Leave empty (\"\") for the default style.\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "QUESTION_PATTERN = \"\"\"\n",
        "- Focus on factual recall and conceptual understanding.\n",
        "- Avoid trivial or trick questions.\n",
        "- Distractors (wrong options) should be plausible but clearly incorrect.\n",
        "- The difficulty should be appropriate for a high-school or undergraduate level.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 5.  HELPER FUNCTIONS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def build_prompt(text: str, pattern: str, n: int) -> str:\n",
        "    \"\"\"Build the instruction prompt sent to the model.\"\"\"\n",
        "    pattern_block = (\n",
        "        f\"\\nAdditional requirements for the questions:\\n{pattern.strip()}\\n\"\n",
        "        if pattern.strip() else \"\"\n",
        "    )\n",
        "\n",
        "    return textwrap.dedent(f\"\"\"\n",
        "        You are an expert educator. Read the following text and generate exactly\n",
        "        {n} multiple-choice questions (MCQs) that test understanding of the content.\n",
        "        {pattern_block}\n",
        "        Return your answer as a **valid JSON array** with no extra text before or\n",
        "        after it.  Each element must follow this exact schema:\n",
        "\n",
        "        {{\n",
        "          \"question\": \"<question text>\",\n",
        "          \"options\": {{\n",
        "            \"a\": \"<option A>\",\n",
        "            \"b\": \"<option B>\",\n",
        "            \"c\": \"<option C>\",\n",
        "            \"d\": \"<option D>\"\n",
        "          }},\n",
        "          \"answer\": \"<correct letter: a | b | c | d>\",\n",
        "          \"explanation\": \"<brief explanation of why the answer is correct>\"\n",
        "        }}\n",
        "\n",
        "        ===TEXT START===\n",
        "        {text.strip()}\n",
        "        ===TEXT END===\n",
        "\n",
        "        JSON array:\n",
        "    \"\"\").strip()\n",
        "\n",
        "\n",
        "def load_model(model_id: str):\n",
        "    \"\"\"Load the tokenizer and model with optional 4-bit quantisation.\"\"\"\n",
        "    print(f\"\\nâ³  Loading model: {model_id}  â€¦\")\n",
        "\n",
        "    # Use 4-bit quantisation when a CUDA GPU is available to save VRAM.\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    bnb_config = None\n",
        "    if use_cuda:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    # Qwen's pad token == eos token by default, which triggers a harmless but\n",
        "    # noisy warning.  Adding a dedicated pad token silences it.\n",
        "    if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
        "        needs_resize = True\n",
        "    else:\n",
        "        needs_resize = False\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\" if use_cuda else None,\n",
        "        torch_dtype=torch.bfloat16 if use_cuda else torch.float32,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model.eval()\n",
        "    if needs_resize:\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "    print(\"âœ…  Model loaded.\\n\")\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def generate_mcqs(tokenizer, model, prompt: str, max_new_tokens: int = 2048) -> str:\n",
        "    \"\"\"Run inference and return the raw model output string.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # --- tokenise input -------------------------------------------------------\n",
        "    # apply_chat_template may return a plain Tensor OR a BatchEncoding object\n",
        "    # (which is NOT a plain dict, so isinstance(x, dict) misses it).\n",
        "    # Safest approach: always ask for a list of ints, then convert manually.\n",
        "    try:\n",
        "        token_list = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=None,      # returns a plain Python list of ints\n",
        "        )\n",
        "        input_ids = torch.tensor([token_list], dtype=torch.long).to(device)\n",
        "    except Exception:\n",
        "        # Fallback: plain tokenisation (no chat template)\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    prompt_len = input_ids.shape[-1]               # save BEFORE generation\n",
        "    attention_mask = torch.ones_like(input_ids)    # all tokens are real (no padding)\n",
        "\n",
        "    # --- generate -------------------------------------------------------------\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,          # greedy â†’ deterministic JSON\n",
        "            temperature=1.0,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode only the newly generated tokens (skip the echoed prompt)\n",
        "    new_tokens = output_ids[0][prompt_len:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def extract_json(raw: str) -> list:\n",
        "    \"\"\"\n",
        "    Extract and parse the first JSON array found in the model output.\n",
        "    Falls back to a regex search if the output contains extra prose.\n",
        "    \"\"\"\n",
        "    # Try direct parse first\n",
        "    raw = raw.strip()\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "\n",
        "    # Search for the first [...] block in the output\n",
        "    match = re.search(r\"\\[.*?\\]\", raw, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            data = json.loads(match.group())\n",
        "            if isinstance(data, list):\n",
        "                return data\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    raise ValueError(\n",
        "        \"Could not parse a JSON array from the model output.\\n\"\n",
        "        f\"Raw output (first 800 chars):\\n{raw[:800]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validate_mcq(mcq: dict, idx: int) -> bool:\n",
        "    \"\"\"Basic sanity check for a single MCQ dict.\"\"\"\n",
        "    required_keys = {\"question\", \"options\", \"answer\", \"explanation\"}\n",
        "    if not required_keys.issubset(mcq.keys()):\n",
        "        print(f\"  âš ï¸  Question {idx+1} is missing keys â€“ skipping.\")\n",
        "        return False\n",
        "    if not isinstance(mcq[\"options\"], dict) or not {\"a\",\"b\",\"c\",\"d\"}.issubset(mcq[\"options\"]):\n",
        "        print(f\"  âš ï¸  Question {idx+1} has malformed options â€“ skipping.\")\n",
        "        return False\n",
        "    if mcq[\"answer\"].lower() not in {\"a\", \"b\", \"c\", \"d\"}:\n",
        "        print(f\"  âš ï¸  Question {idx+1} has an invalid answer key â€“ skipping.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 6.  QUIZ RUNNER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def print_separator(char=\"â”€\", width=60):\n",
        "    print(char * width)\n",
        "\n",
        "\n",
        "def run_quiz(mcqs: list):\n",
        "    \"\"\"Present each MCQ to the user and evaluate their answers.\"\"\"\n",
        "    print_separator(\"â•\")\n",
        "    print(\"ğŸ“  QUIZ TIME!  Answer each question by typing a, b, c, or d.\")\n",
        "    print_separator(\"â•\")\n",
        "\n",
        "    score = 0\n",
        "    total = len(mcqs)\n",
        "\n",
        "    for i, mcq in enumerate(mcqs):\n",
        "        print(f\"\\nQ{i+1} / {total}: {mcq['question']}\\n\")\n",
        "        for letter in [\"a\", \"b\", \"c\", \"d\"]:\n",
        "            print(f\"  {letter})  {mcq['options'][letter]}\")\n",
        "\n",
        "        # Get a valid answer from the user\n",
        "        while True:\n",
        "            raw = input(\"\\nYour answer (a/b/c/d): \").strip().lower()\n",
        "            if raw in {\"a\", \"b\", \"c\", \"d\"}:\n",
        "                break\n",
        "            print(\"  Please type a, b, c, or d.\")\n",
        "\n",
        "        correct = mcq[\"answer\"].lower()\n",
        "\n",
        "        if raw == correct:\n",
        "            score += 1\n",
        "            print(\"\\nâœ…  Correct!\\n\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ  Wrong!  The correct answer was ({correct}) \"\n",
        "                  f\"{mcq['options'][correct]}\")\n",
        "            print(f\"\\nğŸ’¡  Explanation: {mcq['explanation']}\\n\")\n",
        "\n",
        "        print_separator()\n",
        "\n",
        "    # Final score\n",
        "    print(f\"\\nğŸ†  Quiz complete!  You scored {score} / {total}.\")\n",
        "    pct = round(score / total * 100)\n",
        "    if pct == 100:\n",
        "        print(\"    Perfect score â€“ outstanding! ğŸ‰\")\n",
        "    elif pct >= 80:\n",
        "        print(\"    Great job! ğŸ‘\")\n",
        "    elif pct >= 60:\n",
        "        print(\"    Good effort â€“ review the explanations above to improve.\")\n",
        "    else:\n",
        "        print(\"    Keep studying â€“ you'll get there! ğŸ“š\")\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 7.  MAIN\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load model\n",
        "    tokenizer, model = load_model(MODEL_ID)\n",
        "\n",
        "    # Step 2: Build the prompt\n",
        "    prompt = build_prompt(INPUT_TEXT, QUESTION_PATTERN, NUM_QUESTIONS)\n",
        "\n",
        "    # Step 3: Generate MCQs\n",
        "    print(\"âš™ï¸   Generating questions â€¦\\n\")\n",
        "    raw_output = generate_mcqs(tokenizer, model, prompt)\n",
        "\n",
        "    # Step 4: Parse the JSON\n",
        "    try:\n",
        "        mcqs_raw = extract_json(raw_output)\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nâŒ  Error parsing model output:\\n{e}\")\n",
        "        return\n",
        "\n",
        "    # Step 5: Validate and filter\n",
        "    mcqs = [mcq for i, mcq in enumerate(mcqs_raw) if validate_mcq(mcq, i)]\n",
        "\n",
        "    if not mcqs:\n",
        "        print(\"âŒ  No valid questions were generated. \"\n",
        "              \"Try a different model or adjust your input text.\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ…  {len(mcqs)} question(s) generated successfully.\\n\")\n",
        "\n",
        "    # Step 6: Run the interactive quiz\n",
        "    run_quiz(mcqs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b82a097b5322436aaabe574478c66b56",
            "c0ab3315e9d342028e25148ccb6a7447",
            "652507750bfa4491ab694cd537c628e1",
            "c4684b5da6474c2ebb4a88dad9e46d3e",
            "89edbd3468104639afd313fea4263f7d",
            "398ebd1c3bb14392aab7e5a566ae09d1",
            "a82209d733d74455945865a82543b346",
            "5f28826142264e82bd967edf388b461e",
            "9044992109d14587aef5c49246e0b405",
            "26bacfead9c14c38b8f1dc79916b971a",
            "52b8e682b55548b3bba56009d81f8172"
          ]
        },
        "id": "b3UJL492ZITI",
        "outputId": "2cf48573-1239-4d4c-8bd4-6851b27aceb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³  Loading model: Qwen/Qwen2.5-1.5B-Instruct  â€¦\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b82a097b5322436aaabe574478c66b56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ…  Model loaded.\n",
            "\n",
            "âš™ï¸   Generating questions â€¦\n",
            "\n",
            "âœ…  5 question(s) generated successfully.\n",
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "ğŸ“  QUIZ TIME!  Answer each question by typing a, b, c, or d.\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "Q1 / 5: What is the primary function of photosynthesis?\n",
            "\n",
            "  a)  To convert light energy into electrical energy\n",
            "  b)  To provide food for animals\n",
            "  c)  To release oxygen into the atmosphere\n",
            "  d)  To store large amounts of water\n",
            "\n",
            "Your answer (a/b/c/d): b\n",
            "\n",
            "âŒ  Wrong!  The correct answer was (a) To convert light energy into electrical energy\n",
            "\n",
            "ğŸ’¡  Explanation: Photosynthesis primarily converts light energy into chemical energy stored in glucose.\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q2 / 5: In which part of the plant does photosynthesis occur?\n",
            "\n",
            "  a)  Leaves\n",
            "  b)  Stem\n",
            "  c)  Root\n",
            "  d)  All parts equally\n",
            "\n",
            "Your answer (a/b/c/d): a\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q3 / 5: How many molecules of CO2 are required for one molecule of glucose during photosynthesis?\n",
            "\n",
            "  a)  3\n",
            "  b)  4\n",
            "  c)  5\n",
            "  d)  6\n",
            "\n",
            "Your answer (a/b/c/d): d\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q4 / 5: Which substance is not directly involved in the light-dependent reactions of photosynthesis?\n",
            "\n",
            "  a)  ATP\n",
            "  b)  Water\n",
            "  c)  Glucose\n",
            "  d)  Carbon dioxide\n",
            "\n",
            "Your answer (a/b/c/d): d\n",
            "\n",
            "âŒ  Wrong!  The correct answer was (c) Glucose\n",
            "\n",
            "ğŸ’¡  Explanation: Only ATP and NADPH are produced during the light-dependent reactions; glucose is formed in the Calvin cycle.\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Q5 / 5: Why do plants need to take up water from the soil?\n",
            "\n",
            "  a)  To transport nutrients throughout the plant\n",
            "  b)  To cool themselves down\n",
            "  c)  To dissolve sunlight\n",
            "  d)  To collect CO2 from the air\n",
            "\n",
            "Your answer (a/b/c/d): d\n",
            "\n",
            "âœ…  Correct!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ†  Quiz complete!  You scored 3 / 5.\n",
            "    Good effort â€“ review the explanations above to improve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 â€“ install\n",
        "!pip install -q google-generativeai"
      ],
      "metadata": {
        "id": "2YwiazPSb8V7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MCQ Generator using Google Gemini API.\n",
        "Designed to run in Google Colab.\n",
        "\n",
        "Colab setup (run these cells first):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Cell 1 â€“ install the SDK:\n",
        "  !pip install -q google-generativeai\n",
        "\n",
        "Cell 2 â€“ set your API key:\n",
        "  from google.colab import userdata\n",
        "  # Add your key in Colab â†’ Secrets (ğŸ”‘ icon) as  GEMINI_API_KEY\n",
        "  # OR just paste it directly into GEMINI_API_KEY below (not recommended for sharing)\n",
        "\n",
        "Cell 3 â€“ run this script:\n",
        "  !python mcq_generator_gemini.py\n",
        "\n",
        "Get a free API key at: https://aistudio.google.com/app/apikey\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "except ImportError:\n",
        "    raise SystemExit(\"âŒ  Run:  !pip install -q google-generativeai  then restart.\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CONFIGURATION\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Paste your key here, OR store it in Colab Secrets as GEMINI_API_KEY\n",
        "GEMINI_API_KEY = \"\"\n",
        "\n",
        "# Model options:\n",
        "#   \"gemini-2.0-flash\"          â† default: fast & free tier friendly\n",
        "#   \"gemini-2.0-flash-lite\"     â† cheapest / fastest\n",
        "#   \"gemini-1.5-pro\"            â† most capable\n",
        "MODEL = \"gemini-2.0-flash\"\n",
        "\n",
        "NUM_QUESTIONS = 5\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# INPUT TEXT\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "INPUT_TEXT = \"\"\"\n",
        "Photosynthesis is the process used by plants, algae, and certain bacteria to\n",
        "harness energy from sunlight and turn it into chemical energy. During\n",
        "photosynthesis, plants absorb carbon dioxide (CO2) from the air through tiny\n",
        "pores called stomata and water (H2O) from the soil through their roots.\n",
        "Using the energy from sunlight, the plant converts these raw materials into\n",
        "glucose (C6H12O6) and oxygen (O2). The overall chemical equation is:\n",
        "  6CO2 + 6H2O + light energy â†’ C6H12O6 + 6O2\n",
        "Photosynthesis occurs mainly in the chloroplasts, where the green pigment\n",
        "chlorophyll absorbs light energy. The process has two main stages: the\n",
        "light-dependent reactions (which capture solar energy and produce ATP and\n",
        "NADPH) and the Calvin cycle (which uses that energy to fix CO2 into glucose).\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# OPTIONAL PATTERN  â€“ leave empty \"\" to skip\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "QUESTION_PATTERN = \"\"\"\n",
        "- Focus on factual recall and conceptual understanding.\n",
        "- Avoid trivial or trick questions.\n",
        "- Distractors (wrong options) should be plausible but clearly incorrect.\n",
        "- Difficulty: high-school or undergraduate level.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# HELPERS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def resolve_api_key() -> str:\n",
        "    \"\"\"Try Colab Secrets â†’ env var â†’ hardcoded constant.\"\"\"\n",
        "    if GEMINI_API_KEY:\n",
        "        return GEMINI_API_KEY\n",
        "    # Colab Secrets\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        key = userdata.get(\"GEMINI_API_KEY\")\n",
        "        if key:\n",
        "            return key\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Environment variable\n",
        "    key = os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
        "    if key:\n",
        "        return key\n",
        "    raise SystemExit(\n",
        "        \"âŒ  No API key found.\\n\"\n",
        "        \"    Options:\\n\"\n",
        "        \"    1) Add GEMINI_API_KEY in Colab Secrets (ğŸ”‘ icon on the left sidebar)\\n\"\n",
        "        \"    2) Set GEMINI_API_KEY = 'your-key' at the top of this script\\n\"\n",
        "        \"    Get a free key at: https://aistudio.google.com/app/apikey\"\n",
        "    )\n",
        "\n",
        "\n",
        "def build_prompt(text: str, pattern: str, n: int) -> str:\n",
        "    pattern_block = (\n",
        "        f\"\\nAdditional requirements:\\n{pattern.strip()}\\n\"\n",
        "        if pattern.strip() else \"\"\n",
        "    )\n",
        "    return textwrap.dedent(f\"\"\"\n",
        "        You are an expert educator. Read the text below and generate exactly\n",
        "        {n} multiple-choice questions (MCQs) that test understanding of it.\n",
        "        {pattern_block}\n",
        "        Return ONLY a valid JSON array â€“ no markdown fences, no extra text.\n",
        "        Each element must follow this exact schema:\n",
        "        {{\n",
        "          \"question\": \"<question text>\",\n",
        "          \"options\": {{\"a\": \"...\", \"b\": \"...\", \"c\": \"...\", \"d\": \"...\"}},\n",
        "          \"answer\": \"<a | b | c | d>\",\n",
        "          \"explanation\": \"<why the answer is correct>\"\n",
        "        }}\n",
        "\n",
        "        ===TEXT===\n",
        "        {text.strip()}\n",
        "        ===END===\n",
        "    \"\"\").strip()\n",
        "\n",
        "\n",
        "def generate_mcqs(model, prompt: str) -> str:\n",
        "    \"\"\"Call the Gemini API and return raw text.\"\"\"\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config=genai.GenerationConfig(\n",
        "            temperature=0,          # deterministic â†’ clean JSON\n",
        "            max_output_tokens=2048,\n",
        "        ),\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def extract_json(raw: str) -> list:\n",
        "    \"\"\"Parse the first JSON array from the model output.\"\"\"\n",
        "    raw = raw.strip()\n",
        "    # Strip markdown fences if the model adds them anyway\n",
        "    raw = re.sub(r\"^```(?:json)?\\s*\", \"\", raw)\n",
        "    raw = re.sub(r\"\\s*```$\", \"\", raw)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "\n",
        "    match = re.search(r\"\\[.*\\]\", raw, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            data = json.loads(match.group())\n",
        "            if isinstance(data, list):\n",
        "                return data\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    raise ValueError(\n",
        "        \"Could not parse a JSON array from the model output.\\n\"\n",
        "        f\"Raw output (first 800 chars):\\n{raw[:800]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validate_mcq(mcq: dict, idx: int) -> bool:\n",
        "    required = {\"question\", \"options\", \"answer\", \"explanation\"}\n",
        "    if not required.issubset(mcq):\n",
        "        print(f\"  âš ï¸  Q{idx+1} missing keys â€“ skipping.\")\n",
        "        return False\n",
        "    if not isinstance(mcq[\"options\"], dict) or not {\"a\",\"b\",\"c\",\"d\"}.issubset(mcq[\"options\"]):\n",
        "        print(f\"  âš ï¸  Q{idx+1} malformed options â€“ skipping.\")\n",
        "        return False\n",
        "    if mcq[\"answer\"].lower() not in {\"a\",\"b\",\"c\",\"d\"}:\n",
        "        print(f\"  âš ï¸  Q{idx+1} invalid answer key â€“ skipping.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# QUIZ RUNNER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def sep(char=\"â”€\", w=60):\n",
        "    print(char * w)\n",
        "\n",
        "\n",
        "def run_quiz(mcqs: list):\n",
        "    sep(\"â•\")\n",
        "    print(\"ğŸ“  QUIZ TIME!  Type a, b, c, or d to answer each question.\")\n",
        "    sep(\"â•\")\n",
        "\n",
        "    score = 0\n",
        "    for i, mcq in enumerate(mcqs):\n",
        "        print(f\"\\nQ{i+1}/{len(mcqs)}: {mcq['question']}\\n\")\n",
        "        for letter in \"abcd\":\n",
        "            print(f\"  {letter})  {mcq['options'][letter]}\")\n",
        "\n",
        "        while True:\n",
        "            ans = input(\"\\nYour answer (a/b/c/d): \").strip().lower()\n",
        "            if ans in {\"a\",\"b\",\"c\",\"d\"}:\n",
        "                break\n",
        "            print(\"  Please type a, b, c, or d.\")\n",
        "\n",
        "        correct = mcq[\"answer\"].lower()\n",
        "        if ans == correct:\n",
        "            score += 1\n",
        "            print(\"\\nâœ…  Correct!\\n\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ  Wrong!  Correct answer: ({correct}) {mcq['options'][correct]}\")\n",
        "            print(f\"ğŸ’¡  {mcq['explanation']}\\n\")\n",
        "        sep()\n",
        "\n",
        "    print(f\"\\nğŸ†  Score: {score}/{len(mcqs)}\")\n",
        "    pct = score / len(mcqs) * 100\n",
        "    if pct == 100:  print(\"    Perfect! ğŸ‰\")\n",
        "    elif pct >= 80: print(\"    Great job! ğŸ‘\")\n",
        "    elif pct >= 60: print(\"    Good effort â€“ review the explanations above.\")\n",
        "    else:           print(\"    Keep studying! ğŸ“š\")\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# MAIN\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def main():\n",
        "    # 1. Auth\n",
        "    api_key = resolve_api_key()\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(MODEL)\n",
        "    print(f\"ğŸ¤–  Model  : {MODEL}\")\n",
        "    print(f\"ğŸ“‹  Generating {NUM_QUESTIONS} questions â€¦\\n\")\n",
        "\n",
        "    # 2. Generate\n",
        "    prompt = build_prompt(INPUT_TEXT, QUESTION_PATTERN, NUM_QUESTIONS)\n",
        "    try:\n",
        "        raw = generate_mcqs(model, prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ  Gemini API error: {e}\")\n",
        "        return\n",
        "\n",
        "    # 3. Parse\n",
        "    try:\n",
        "        mcqs_raw = extract_json(raw)\n",
        "    except ValueError as e:\n",
        "        print(f\"âŒ  {e}\")\n",
        "        return\n",
        "\n",
        "    # 4. Validate\n",
        "    mcqs = [m for i, m in enumerate(mcqs_raw) if validate_mcq(m, i)]\n",
        "    if not mcqs:\n",
        "        print(\"âŒ  No valid questions generated.\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ…  {len(mcqs)} question(s) ready.\\n\")\n",
        "\n",
        "    # 5. Quiz\n",
        "    run_quiz(mcqs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "A2qQtZcvb8TT",
        "outputId": "91a6ddd1-3e39-4c97-fc8a-725d5a764973"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤–  Model  : gemini-2.0-flash\n",
            "ğŸ“‹  Generating 5 questions â€¦\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.0-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 984.55ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ  Gemini API error: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
            "Please retry in 27.020577868s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MCQ Generator using Google Gemini API (new google-genai SDK).\n",
        "Designed to run in Google Colab.\n",
        "\n",
        "Colab setup:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Cell 1 â€“ install the NEW SDK:\n",
        "  !pip install -q google-genai\n",
        "\n",
        "Cell 2 â€“ add your key in Colab Secrets (ğŸ”‘ left sidebar)\n",
        "  Name: GEMINI_API_KEY\n",
        "\n",
        "Cell 3 â€“ run:\n",
        "  !python mcq_generator_gemini.py\n",
        "\n",
        "Free key â†’ https://aistudio.google.com/app/apikey\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "import os\n",
        "import time\n",
        "\n",
        "try:\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "except ImportError:\n",
        "    raise SystemExit(\"âŒ  Run:  !pip install -q google-genai  then restart the runtime.\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CONFIGURATION\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Paste your key here OR store it in Colab Secrets as GEMINI_API_KEY\n",
        "GEMINI_API_KEY = \"\"\n",
        "\n",
        "# Model options (free tier):\n",
        "#   \"gemini-2.0-flash-lite\"   â† most generous free quota  âœ… recommended\n",
        "#   \"gemini-2.0-flash\"        â† fast, slightly lower free quota\n",
        "#   \"gemini-1.5-flash\"        â† older but very stable\n",
        "MODEL = \"gemini-2.0-flash-lite\"\n",
        "\n",
        "NUM_QUESTIONS = 5\n",
        "\n",
        "# Retry settings for 429 / quota errors\n",
        "MAX_RETRIES = 4          # number of retry attempts\n",
        "RETRY_WAIT  = 30         # seconds to wait between retries (if no hint in error)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# INPUT TEXT\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "INPUT_TEXT = \"\"\"\n",
        "Photosynthesis is the process used by plants, algae, and certain bacteria to\n",
        "harness energy from sunlight and turn it into chemical energy. During\n",
        "photosynthesis, plants absorb carbon dioxide (CO2) from the air through tiny\n",
        "pores called stomata and water (H2O) from the soil through their roots.\n",
        "Using the energy from sunlight, the plant converts these raw materials into\n",
        "glucose (C6H12O6) and oxygen (O2). The overall chemical equation is:\n",
        "  6CO2 + 6H2O + light energy â†’ C6H12O6 + 6O2\n",
        "Photosynthesis occurs mainly in the chloroplasts, where the green pigment\n",
        "chlorophyll absorbs light energy. The process has two main stages: the\n",
        "light-dependent reactions (which capture solar energy and produce ATP and\n",
        "NADPH) and the Calvin cycle (which uses that energy to fix CO2 into glucose).\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# OPTIONAL PATTERN  â€“ leave \"\" to skip\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "QUESTION_PATTERN = \"\"\"\n",
        "- Focus on factual recall and conceptual understanding.\n",
        "- Avoid trivial or trick questions.\n",
        "- Distractors (wrong options) should be plausible but clearly incorrect.\n",
        "- Difficulty: high-school or undergraduate level.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# HELPERS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def resolve_api_key() -> str:\n",
        "    \"\"\"Try hardcoded constant â†’ Colab Secrets â†’ environment variable.\"\"\"\n",
        "    if GEMINI_API_KEY:\n",
        "        return GEMINI_API_KEY\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        key = userdata.get(\"GEMINI_API_KEY\")\n",
        "        if key:\n",
        "            return key\n",
        "    except Exception:\n",
        "        pass\n",
        "    key = os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
        "    if key:\n",
        "        return key\n",
        "    raise SystemExit(\n",
        "        \"âŒ  No API key found.\\n\"\n",
        "        \"    1) Add GEMINI_API_KEY in Colab Secrets (ğŸ”‘ left sidebar)\\n\"\n",
        "        \"    2) OR paste it into GEMINI_API_KEY at the top of this script.\\n\"\n",
        "        \"    Free key â†’ https://aistudio.google.com/app/apikey\"\n",
        "    )\n",
        "\n",
        "\n",
        "def build_prompt(text: str, pattern: str, n: int) -> str:\n",
        "    pattern_block = (\n",
        "        f\"\\nAdditional requirements:\\n{pattern.strip()}\\n\"\n",
        "        if pattern.strip() else \"\"\n",
        "    )\n",
        "    return textwrap.dedent(f\"\"\"\n",
        "        You are an expert educator. Read the text below and generate exactly\n",
        "        {n} multiple-choice questions (MCQs) that test understanding of it.\n",
        "        {pattern_block}\n",
        "        Return ONLY a valid JSON array â€“ no markdown fences, no extra text.\n",
        "        Each element must follow this exact schema:\n",
        "        {{\n",
        "          \"question\": \"<question text>\",\n",
        "          \"options\": {{\"a\": \"...\", \"b\": \"...\", \"c\": \"...\", \"d\": \"...\"}},\n",
        "          \"answer\": \"<a | b | c | d>\",\n",
        "          \"explanation\": \"<why the answer is correct>\"\n",
        "        }}\n",
        "\n",
        "        ===TEXT===\n",
        "        {text.strip()}\n",
        "        ===END===\n",
        "    \"\"\").strip()\n",
        "\n",
        "\n",
        "def call_gemini(client: genai.Client, prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Call the Gemini API with automatic retry on 429 quota errors.\n",
        "    Parses the retry-delay hint from the error message when available.\n",
        "    \"\"\"\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=MODEL,\n",
        "                contents=prompt,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=0,\n",
        "                    max_output_tokens=2048,\n",
        "                ),\n",
        "            )\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            err = str(e)\n",
        "            is_quota = \"429\" in err or \"quota\" in err.lower() or \"rate\" in err.lower()\n",
        "\n",
        "            if is_quota and attempt < MAX_RETRIES:\n",
        "                # Read the suggested wait time from the error if present\n",
        "                wait = RETRY_WAIT\n",
        "                m = re.search(r\"retry in ([\\d.]+)s\", err, re.IGNORECASE)\n",
        "                if m:\n",
        "                    wait = int(float(m.group(1))) + 2   # small buffer\n",
        "                print(f\"  â³  Quota limit hit â€“ waiting {wait}s \"\n",
        "                      f\"(attempt {attempt}/{MAX_RETRIES}) â€¦\")\n",
        "                time.sleep(wait)\n",
        "            else:\n",
        "                raise   # non-quota error or retries exhausted\n",
        "\n",
        "\n",
        "def extract_json(raw: str) -> list:\n",
        "    raw = raw.strip()\n",
        "    # Strip markdown fences the model might add despite instructions\n",
        "    raw = re.sub(r\"^```(?:json)?\\s*\", \"\", raw)\n",
        "    raw = re.sub(r\"\\s*```$\", \"\", raw)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "\n",
        "    # Fallback: find the first [...] block\n",
        "    match = re.search(r\"\\[.*\\]\", raw, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            data = json.loads(match.group())\n",
        "            if isinstance(data, list):\n",
        "                return data\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    raise ValueError(\n",
        "        \"Could not parse a JSON array from the model output.\\n\"\n",
        "        f\"Raw output (first 800 chars):\\n{raw[:800]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validate_mcq(mcq: dict, idx: int) -> bool:\n",
        "    required = {\"question\", \"options\", \"answer\", \"explanation\"}\n",
        "    if not required.issubset(mcq):\n",
        "        print(f\"  âš ï¸  Q{idx+1} missing keys â€“ skipping.\")\n",
        "        return False\n",
        "    if not isinstance(mcq[\"options\"], dict) or not {\"a\",\"b\",\"c\",\"d\"}.issubset(mcq[\"options\"]):\n",
        "        print(f\"  âš ï¸  Q{idx+1} malformed options â€“ skipping.\")\n",
        "        return False\n",
        "    if mcq[\"answer\"].lower() not in {\"a\",\"b\",\"c\",\"d\"}:\n",
        "        print(f\"  âš ï¸  Q{idx+1} invalid answer key â€“ skipping.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# QUIZ RUNNER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def sep(char=\"â”€\", w=60):\n",
        "    print(char * w)\n",
        "\n",
        "\n",
        "def run_quiz(mcqs: list):\n",
        "    sep(\"â•\")\n",
        "    print(\"ğŸ“  QUIZ TIME!  Type a, b, c, or d to answer each question.\")\n",
        "    sep(\"â•\")\n",
        "\n",
        "    score = 0\n",
        "    for i, mcq in enumerate(mcqs):\n",
        "        print(f\"\\nQ{i+1}/{len(mcqs)}: {mcq['question']}\\n\")\n",
        "        for letter in \"abcd\":\n",
        "            print(f\"  {letter})  {mcq['options'][letter]}\")\n",
        "\n",
        "        while True:\n",
        "            ans = input(\"\\nYour answer (a/b/c/d): \").strip().lower()\n",
        "            if ans in {\"a\",\"b\",\"c\",\"d\"}:\n",
        "                break\n",
        "            print(\"  Please type a, b, c, or d.\")\n",
        "\n",
        "        correct = mcq[\"answer\"].lower()\n",
        "        if ans == correct:\n",
        "            score += 1\n",
        "            print(\"\\nâœ…  Correct!\\n\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ  Wrong!  Correct answer: ({correct}) {mcq['options'][correct]}\")\n",
        "            print(f\"ğŸ’¡  {mcq['explanation']}\\n\")\n",
        "        sep()\n",
        "\n",
        "    print(f\"\\nğŸ†  Score: {score}/{len(mcqs)}\")\n",
        "    pct = score / len(mcqs) * 100\n",
        "    if pct == 100:  print(\"    Perfect! ğŸ‰\")\n",
        "    elif pct >= 80: print(\"    Great job! ğŸ‘\")\n",
        "    elif pct >= 60: print(\"    Good effort â€“ review the explanations above.\")\n",
        "    else:           print(\"    Keep studying! ğŸ“š\")\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# MAIN\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def main():\n",
        "    api_key = resolve_api_key()\n",
        "    client  = genai.Client(api_key=api_key)\n",
        "\n",
        "    print(f\"ğŸ¤–  Model  : {MODEL}\")\n",
        "    print(f\"ğŸ“‹  Generating {NUM_QUESTIONS} questions â€¦\\n\")\n",
        "\n",
        "    prompt = build_prompt(INPUT_TEXT, QUESTION_PATTERN, NUM_QUESTIONS)\n",
        "\n",
        "    try:\n",
        "        raw = call_gemini(client, prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ  Gemini API error: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        mcqs_raw = extract_json(raw)\n",
        "    except ValueError as e:\n",
        "        print(f\"âŒ  {e}\")\n",
        "        return\n",
        "\n",
        "    mcqs = [m for i, m in enumerate(mcqs_raw) if validate_mcq(m, i)]\n",
        "    if not mcqs:\n",
        "        print(\"âŒ  No valid questions generated.\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ…  {len(mcqs)} question(s) ready.\\n\")\n",
        "    run_quiz(mcqs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "Nzdbf0Qxb8QD",
        "outputId": "14baa1bc-0626-4f53-8119-7cd0befc29d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤–  Model  : gemini-2.0-flash-lite\n",
            "ğŸ“‹  Generating 5 questions â€¦\n",
            "\n",
            "  â³  Quota limit hit â€“ waiting 20s (attempt 1/4) â€¦\n",
            "  â³  Quota limit hit â€“ waiting 60s (attempt 2/4) â€¦\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-465806600.py\u001b[0m in \u001b[0;36mcall_gemini\u001b[0;34m(client, prompt)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             response = client.models.generate_content(\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5473\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5474\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5475\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4214\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   4215\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     )\n\u001b[0;32m-> 1386\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m     response_body = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1198\u001b[0m       )\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       return HttpResponse(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\\nPlease retry in 58.260108659s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions':...",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-465806600.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-465806600.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_gemini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âŒ  Gemini API error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-465806600.py\u001b[0m in \u001b[0;36mcall_gemini\u001b[0;34m(client, prompt)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 print(f\"  â³  Quota limit hit â€“ waiting {wait}s \"\n\u001b[1;32m    156\u001b[0m                       f\"(attempt {attempt}/{MAX_RETRIES}) â€¦\")\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mraise\u001b[0m   \u001b[0;31m# non-quota error or retries exhausted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQHnwmvVb8NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1iE0Cn9ayIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}